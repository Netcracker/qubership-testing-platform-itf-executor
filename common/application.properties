##=============DataBase Settings (ToDo: check unused)===========================
spring.datasource.url=${JDBC_URL:jdbc:postgresql://localhost:5432/atp-itf-executor}
spring.datasource.username=${ITF_EXECUTOR_DB_USER:postgres}
spring.datasource.password=${ITF_EXECUTOR_DB_PASSWORD:postgres}
spring.datasource.driver-class-name=org.postgresql.Driver
spring.datasource.type=javax.sql.DataSource

spring.datasource.hikari.minimum-idle=${SPRING_DATASOURCE_MINIDLE:10}
spring.datasource.hikari.maximum-pool-size=${SPRING_DATASOURCE_MAXTOTAL:150}
spring.datasource.hikari.idle-timeout=${SPRING_DATASOURCE_HIKARI_IDLE_TIMEOUT:55000}
spring.datasource.hikari.max-lifetime=${SPRING_DATASOURCE_HIKARI_MAX_LIFETIME:3960000}

org.hibernate=hibernate-java8
hibernate.hbm2ddl.auto=none
spring.jpa.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.jdbc.lob.non_contextual_creation=true
hibernate.second.level.cache.enabled=true
hibernate.query.cache.enabled=true
hibernate.generate.statistics=true
hibernate.session.events.log=false
##==============================Spring bean==============================
spring.main.allow-bean-definition-overriding=true
spring.main.allow-circular-references=true
spring.mvc.pathmatch.matching-strategy = ANT_PATH_MATCHER
##==============================Undertow===============================
server.port=${HTTP_PORT:8080}
embedded.https.enabled=${EMBEDDED_HTTPS_ENABLED:true}
embedded.tls.server.port=${EMBEDDED_TLS_SERVER_PORT:9443}
embedded.ssl.server.port=${EMBEDDED_SSL_SERVER_PORT:8443}
keystore.password=${KEYSTORE_PASSWORD:changeit}
# Undertow session timeout (in minutes) before authorization is expired
server.servlet.session.timeout=${UNDERTOW_SESSION_TIMEOUT:58m}
server.forward-headers-strategy=NATIVE
server.compression.enabled=${UNDERTOW_COMPRESSION_ENABLED:false}
server.compression.mime-types=${UNDERTOW_COMPRESSION_MIMETYPE:text/html,text/xml,text/event-stream,text/plain,text/css,text/javascript,application/javascript,application/json,application/xml}
server.undertow.threads.io=${SERVER_UNDERTOW_IO_THREADS:4}
server.undertow.threads.worker=${SERVER_UNDERTOW_WORKER_THREADS:32}
server.undertow.accesslog.enabled=${SERVER_UNDERTOW_ACCESSLOG_ENABLED:true}
server.undertow.accesslog.dir=/itf/logs
#server.undertow.accesslog.pattern=common
#server.undertow.accesslog.pattern=combined
server.undertow.accesslog.pattern=%h %u %t "%r" %s %b %D %T
server.undertow.accesslog.prefix=access_log.
server.undertow.accesslog.rotate=true
server.undertow.accesslog.suffix=log
server.undertow.options.server.record-request-start-time=true
server.undertow.options.server.enable-statistics=true
jboss.threads.eqe.statistics=${JBOSS_THREADS_EQE_STATISTICS:true}
##===============================ATP ITF===============================
#Parameter for Java GC to avoid the uncontrolled memory consumption
max.ram.size=${MAX_RAM_SIZE:3000m}
lock.provider.process.timeout=25000
session.handler.process.timeout=25000
file.encoding=UTF-8

working.directory=itf/custom-storage
jpda.enabled=${JPDA_ENABLED:false}
#====================JMS connection properties=========================
# values for max attempts - unlimited or numerical value
jms.connection.recovery.interval=5000
jms.connection.max.attempts=unlimited
##===============================Logging===============================
logging.level.root=${LOG_LEVEL:INFO}
log.graylog.on=${GRAYLOG_ON:false}
log.graylog.host=${GRAYLOG_HOST:graylog-service-address}
log.graylog.port=${GRAYLOG_PORT:12201}
log.graylog.level=INFO
log.appender.date.format=dd.MM.yy HH:mm:ss
atp.logging.business.keys=projectId,executionRequestId,testRunId,callChainId,itfSessionId,itfContextId,traceId
##==================Integration with Spring Cloud======================
spring.application.name=${SERVICE_NAME:atp-itf-executor}
eureka.client.enabled=${EUREKA_CLIENT_ENABLED:true}
eureka.instance.prefer-ip-address=true
eureka.serviceUrl.default=http://atp-registry-service-service-address/eureka
eureka.client.serviceUrl.defaultZone=${SERVICE_REGISTRY_URL:http://atp-registry-service-address/eureka}
## disable eureka client logging (version conflict)
eureka.client.healthcheck.enabled=false
spring.sleuth.messaging.jms.enabled=false
spring.cloud.config.discovery.enabled=true
##==================Integration with ATP======================
atp.service.public=${ATP_SERVICE_PUBLIC:true}
atp.service.internal=${ATP_SERVICE_INTERNAL:true}
atp.service.path=${ATP_SERVICE_PATH:/api/atp-itf-executor/v1/**}
##==================atp-auth-spring-boot-starter=======================
spring.profiles.active=${SPRING_PROFILES:default}
spring.cache.cache-names=projects
spring.cache.caffeine.spec=maximumSize=100, expireAfterAccess=120s, expireAfterWrite=120s
keycloak.enabled=${KEYCLOAK_ENABLED:true}
keycloak.resource=${KEYCLOAK_CLIENT_NAME:atp-itf}
keycloak.credentials.secret=${KEYCLOAK_SECRET:71b6a213-e3b0-4bf4-86c8-dfe11ce9e248}
keycloak.bearer-only=true
keycloak.realm=${KEYCLOAK_REALM:atp2}
keycloak.auth-server-url=${KEYCLOAK_AUTH_URL:https://atp-keycloak-service-address/auth}
atp-auth.project_info_endpoint=${PROJECT_INFO_ENDPOINT:/api/v1/users/projects}
atp-auth.enable-m2m=true
atp-auth.headers.content-security-policy=${CONTENT_SECURITY_POLICY:default-src 'self' 'unsafe-inline' *}
##=============================Feign===================================
atp.public.gateway.url=${ATP_PUBLIC_GATEWAY_URL:https://atp-public-gateway-service-address}
atp.internal.gateway.url=${ATP_INTERNAL_GATEWAY_URL:http://atp-internal-gateway-service-address}
atp.internal.gateway.enabled=${ATP_INTERNAL_GATEWAY_ENABLED:true}
atp.internal.gateway.name=${ATP_INTERNAL_GATEWAY_NAME:atp-internal-gateway}
feign.httpclient.disableSslValidation=${FEIGN_HTTPCLIENT_DISABLE_SSL:true}
feign.httpclient.enabled=${FEIGN_HTTPCLIENT_ENABLED:false}
feign.okhttp.enabled=${FEIGN_OKHTTP_ENABLED:true}
## datasets
feign.atp.datasets.name=${FEIGN_ATP_DATASETS_NAME:ATP-DATASETS}
feign.atp.datasets.url=${FEIGN_ATP_DATASETS_URL:}
feign.atp.datasets.route=${FEIGN_ATP_DATASETS_ROUTE:api/atp-datasets/v1}
## bulk validator
feign.atp.bv.name=${FEIGN_ATP_BV_NAME:ATP-BV}
feign.atp.bv.url=${FEIGN_ATP_BV_URL:}
feign.atp.bv.route=${FEIGN_ATP_BV_ROUTE:api/bvtool/v1}
## environments
feign.atp.environments.name=${FEIGN_ATP_ENVIRONMENTS_NAME:ATP-ENVIRONMENTS}
feign.atp.environments.url=${FEIGN_ATP_ENVIRONMENTS_URL:}
feign.atp.environments.route=${FEIGN_ATP_ENVIRONMENTS_ROUTE:api/atp-environments/v1}
## catalogue
feign.atp.catalogue.name=${FEIGN_ATP_CATALOGUE_NAME:ATP-CATALOGUE}
feign.atp.catalogue.route=${FEIGN_ATP_CATALOGUE_ROUTE:api/atp-catalogue/v1}
feign.atp.catalogue.url=${FEIGN_ATP_CATALOGUE_URL:}
## users
feign.atp.users.url=${FEIGN_ATP_USERS_URL:}
feign.atp.users.name=${FEIGN_ATP_USERS_NAME:ATP-USERS-BACKEND}
feign.atp.users.route=${FEIGN_ATP_USERS_ROUTE:api/atp-users-backend/v1}
## itf executor
feign.atp.executor.name=${FEIGN_ATP_ITF_EXECUTOR_NAME:ATP-ITF-EXECUTOR}
feign.atp.executor.url=${FEIGN_ATP_ITF_EXECUTOR_URL:}
feign.atp.executor.route=${FEIGN_ATP_ITF_EXECUTOR_ROUTE:api/atp-itf-executor/v1}
## itf reports
feign.atp.reports.name=${FEIGN_ATP_ITF_REPORTS_NAME:ATP-ITF-REPORTS}
feign.atp.reports.url=${FEIGN_ATP_ITF_REPORTS_URL:}
feign.atp.reports.route=${FEIGN_ATP_ITF_REPORTS_ROUTE:api/atp-itf-reports/v1}
## ei
feign.atp.ei.name=${FEIGN_ATP_ITF_EI_NAME:ATP-ITF-EI}
feign.atp.ei.url=${FEIGN_ATP_ITF_EI_URL:}
feign.atp.ei.route=${FEIGN_ATP_ITF_EI_ROUTE:api/atp-itf-ei/v1}
##========================Feign timeout================================
feign.client.config.default.connectTimeout=${FEIGN_CONNECT_TIMEOUT:160000000}
feign.client.config.default.readTimeout=${FEIGN_READ_TIMEOUT:160000000}
##==================GridFS==============================================
ei.gridfs.database=${EI_GRIDFS_DB:local_itf_gridfs}
ei.gridfs.host=${EI_GRIDFS_DB_ADDR:localhost}
ei.gridfs.port=${EI_GRIDFS_DB_PORT:27017}
ei.gridfs.user=${EI_GRIDFS_USER:admin}
ei.gridfs.password=${EI_GRIDFS_PASSWORD:admin}
##=============================Kafka===================================
kafka.enable=${KAFKA_ENABLE:true}
kafka.topic=${KAFKA_TOPIC:catalog_notification_topic}
kafka.topic.itf.lite.export.start=${KAFKA_ITF_LITE_EXPORT_ITF_START_TOPIC:itf_lite_to_itf_export_start}
kafka.topic.itf.lite.export.finish=${KAFKA_ITF_LITE_EXPORT_ITF_FINISH_TOPIC:itf_lite_to_itf_export_finish}
kafka.topic.environments_notification=${KAFKA_ENVIRONMENTS_NOTIFICATION_TOPIC:environments_notification_topic}
kafka.service.entities.topic=${KAFKA_SERVICE_ENTITIES_TOPIC:service_entities}
kafka.service.entities.topic.partitions=${KAFKA_SERVICE_ENTITIES_TOPIC_PARTITIONS:1}
kafka.service.entities.topic.replicas=${KAFKA_SERVICE_ENTITIES_TOPIC_REPLICATION_FACTOR:3}
service.entities.migration.enabled=${SERVICE_ENTITIES_MIGRATION_ENABLED:true}
spring.kafka.producer.bootstrap-servers=${KAFKA_SERVERS:kafka.kafka-cluster.svc:9092}
spring.kafka.consumer.bootstrap-servers=${KAFKA_SERVERS:kafka.kafka-cluster.svc:9092}
spring.kafka.consumer.client-id=${KAFKA_CLIENT_ID:atp-itf-executor}
spring.kafka.consumer.group-id=${KAFKA_GROUP_ID:atp-itf-executor}
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.producer.request.timeout.ms=60000
##==================Zipkin=====================
spring.sleuth.enabled=${ZIPKIN_ENABLE:false}
spring.sleuth.sampler.probability=${ZIPKIN_PROBABILITY:1.0}
spring.zipkin.baseUrl=${ZIPKIN_URL:http://localhost:9411}
spring.sleuth.web.additional-skip-pattern=/actuator/health

##=============================ActiveMQ===============================
# Failover URL is commented till infinite reconnections problem is not solved
##message-broker.url=${ATP_ITF_BROKER_URL_TCP:failover:(tcp://atp-activemq.dev04.svc:61616?wireFormat.maxInactivityDuration=0&wireFormat.maxFrameSize=104857600&keepAlive=true)?timeout=3000&initialReconnectDelay=25&maxReconnectAttempts=100&maxReconnectDelay=30000&useExponentialBackOff=true}
##message-broker.url.ws=failover:(${ATP_ITF_BROKER_URL_WS:ws://atp-activemq:61614})
#Tested on atp-cloud/ocs (may be decided to set via base pack):
##message-broker.url=nio://atp-activemq.ocs.svc:61616?wireFormat.maxInactivityDuration=30000

message-broker.url=${ATP_ITF_BROKER_URL_TCP:tcp://atp-activemq:61616?wireFormat.maxInactivityDuration=0&wireFormat.maxFrameSize=104857600}
message-broker.url.ws=${ATP_ITF_BROKER_URL_WS:ws://atp-activemq:61614}
message-broker.useCompression=false
message-broker.useAsyncSend=true
message-broker.queuePrefetch=1

message-broker.configurator-executor-event-triggers.topic=${CONFIGURATOR_EXECUTOR_EVENT_TRIGGERS_TOPIC:configurator_executor_event_triggers}
message-broker.executor-configurator-event-triggers.topic=${EXECUTOR_CONFIGURATOR_EVENT_TRIGGERS_TOPIC:executor_configurator_event_triggers}
message-broker.executor-stubs-sync.topic=${EXECUTOR_STUBS_SYNC_TOPIC:executor_stubs_sync}
message-broker.configurator-stubs.topic=${CONFIGURATOR_STUBS_TOPIC:configurator_stubs}
message-broker.stubs-configurator.topic=${STUBS_CONFIGURATOR_TOPIC:stubs_configurator}
message-broker.end-exceptional-situations-events.topic=${END_EXCEPTIONAL_SITUATIONS_EVENTS_TOPIC:end_exceptional_situations_events}
message-broker.executor-tccontext-operations.topic=${EXECUTOR_TCCONTEXT_OPERATIONS_TOPIC:executor_tccontext_operations}
message-broker.executor-disabling-stepbystep.topic=${EXECUTOR_DISABLING_STEPBYSTEP_TOPIC:executor_disabling_stepbystep}
message-broker.executor-sync-reload-dictionary.topic=${EXECUTOR_SYNC_RELOAD_DICTIONARY_TOPIC:executor_sync_reload_dictionary}
message-broker.eds-update.topic=${EDS_UPDATE_TOPIC:eds_update}

message-broker.stubs-executor-incoming-request.queue=${STUBS_EXECUTOR_INCOMING_QUEUE:stubs_executor_incoming_request}
message-broker.executor-stubs-outgoing-response.queue=${EXECUTOR_STUBS_OUTGOING_QUEUE:executor_stubs_outgoing_response}
message-broker.reports.queue=${REPORT_QUEUE:ReportExecution}

message-broker.end-exceptional-situations-events.message-time-to-live=${END_EXCEPTIONAL_SITUATIONS_EVENTS_MESSAGES_TTL:180000}
message-broker.other-topics.message-time-to-live=${OTHER_TOPICS_MESSAGES_TTL:180000}
message-broker.executor-stubs-outgoing-response.message-time-to-live=${EXECUTOR_STUBS_OUTGOING_MESSAGES_TTL:90000}
message-broker.reports.message-time-to-live=${REPORTS_MESSAGES_TTL:1800000}

message-broker.reports.useCompression=${REPORT_USE_COMPRESSION:true}
message-broker.reports.useAsyncSend=${REPORT_USE_ASYNC_SEND:true}
message-broker.reports.maxThreadPoolSize=${REPORT_MAX_THREAD_POOL_SIZE:1200}
message-broker.reports.connectionsPoolSize=40
message-broker.stubs-executor.listenerContainerFactory.concurrency=${STUBS_EXECUTOR_CONCURRENCY:120-900}
message-broker.stubs-executor.listenerContainerFactory.maxMessagesPerTask=${STUBS_EXECUTOR_MAX_MESSAGES_PER_TASK:-1}
##======================ATP Services integration=======================
dataset.service.url=${DATASET_SERVICE_URL:https://atp-dataset-service-address}
environments.service.url=${ENVIRONMENTS_SERVICE_URL:http://atp-environments-service-address}
atp.catalogue.url=${CATALOGUE_URL:http://atp-catalogue-service-address}
configurator.url=${ATP_ITF_CONFIGURATOR_URL:http://atp-itf-configurator-service-address}
bv.service.url=${BV_SERVICE_URL:http://atp-bv-service-address}
ei.service.url=${EI_SERVICE_URL:http://atp-ei-service-address}
##========================Spring Boot Actuator=========================
management.endpoint.env.enabled=true
management.endpoint.shutdown.enabled=true
##======================Hazelcast configurations=======================
hazelcast.cache.enabled=${HAZELCAST_CACHE_ENABLED:true}
hazelcast.client.name=atp-itf-executor
hazelcast.cluster-name=atp-hc
hazelcast.address=${HAZELCAST_ADDRESS:127.0.0.1:5701}
hazelcast.internal.map.expiration.task.period.seconds=${HAZELCAST_EXPIRATION_TASK_PERIOD_SECONDS:5}
hazelcast.internal.map.expiration.cleanup.percentage=${HAZELCAST_EXPIRATION_CLEANUP_PERCENTAGE:10}
hazelcast.project-settings.cache.refill.time.seconds=${HAZELCAST_PROJECT_SETTINGS_CACHE_REFILL_TIME_SECONDS:3600}
##=============================Ram Adapter properties=============================
ram.adapter.type=kafka
##=============================Interceptors=============================
interceptors.folder=./interceptors
##=============================Export/Import=============================
ei.folder=data/ei-session
ei.waiting.project.creation.timeout=5000
ei.waiting.project.creation.attempt.count=5
atp.export.workdir=${EI_CLEAN_JOB_WORKDIR:exportimport/node}
atp.ei.file.cleanup.job.enable=${EI_CLEAN_JOB_ENABLED:true}
atp.ei.file.cleanup.job.fixedRate=${EI_CLEAN_SCHEDULED_JOB_PERIOD_MS:86400000}
atp.ei.file.delete.after.ms=${EI_CLEAN_JOB_FILE_DELETE_AFTER_MS:172800000}
##=============================Report=============================
report.producer.useGroupingMessages=false
report.producer.warnAboutSize=5000000
report.producer.maxSize=6000000
report.execution.enabled=true
report.in.different.thread=false
report.execution.sender.thread.pool.size=10
##=============================Transports=============================
transport.folder=./transports
load.transports.at.startup=all
#Custom transport lib
transport.lib=${TRANSPORT_CUSTOM_LIB_FOLDER:}
#Outbound smpp transport
transport.smpp.bind.timeout=5000
transport.smpp.connect.timeout=10000
#Package logging and package hex dump logging
transport.smpp.logging.hex.dump=false
transport.smpp.logging.package=false
##=============================TCP dump settings=============================
tcpdump.folder=tcpdump
#Default value tcpdump.folder=tcpdump (ROOT_ITF4_FOLDER\tcpdump)
#Examples:
#tcpdump.folder=..\tcpdump - will create the "tcpdump" folder 1 level higher (from ROOT_ITF4_FOLDER)
#tcpdump.folder=..\..\tcpdump - will create the "tcpdump" folder 2 level higher (from ROOT_ITF4_FOLDER)
##=============================Executor settings=============================
#Size of the thread pool which is used in Execution
executor.thread.pool.size=250
executor.thread.pool.core.size=50
executor.context.start.max.time=40000
executor.context.from.atp.finish.max.time=1800000
executor.context.check.interval=10000

#Size of the thread pool which is used in ATP Reporting
background.executor.thread.pool.size=10
infinite.loop.protection.barrier=100
# Cache timeout for tc/sp context differences reporting to RAM2. Should not be increased unless there are big configured delays on situations
tc.context_diff_cache.timeout_minutes=10
# Stub transaction duration time limit (empty, 0 or negative means "no limit" - default behavior)
stubs.processing.duration.time.max=0
##=============================File Uploader Settings=============================
local.storage.directory=data
eds.gridfs.enabled=${EDS_GRIDFS_ENABLED:true}
eds.gridfs.host=${MONGO_DB_ADDR:localhost}
eds.gridfs.port=${MONGO_DB_PORT:27017}
eds.gridfs.database=${EDS_GRIDFS_DB:local_itf_gridfs}
eds.gridfs.username=${EDS_GRIDFS_USER:admin}
eds.gridfs.password=${EDS_GRIDFS_PASSWORD:admin}
spring.servlet.multipart.max-file-size=${SPRING_SERVLET_MULTIPART_MAX_FILE_SIZE:256MB}
spring.servlet.multipart.max-request-size=${SPRING_SERVLET_MULTIPART_MAX_REQUEST_SIZE:256MB}
ui.uploadForm.maxAllowFileSize=268435456
scheduled.cleanup.tempFiles.fixedRate.ms=${SCHEDULED_CLEANUP_TEMPFILES_FIXEDRATE_MS:3600000}
scheduled.cleanup.tempFiles.modifiedBefore.ms=${SCHEDULED_CLEANUP_TEMPFILES_MODIFIED_BEFORE_MS:3600000}
##=============================Server-Side Events settings=============================
atp-itf-executor.sse-timeout=${ATP_ITF_EXECUTOR_SSE_TIMEOUT:60000}
atp-itf-executor.sse-reconnect-time=6000
##=============================Diameter-transport configurable settings (they are not used in the ITF itself, so can be set directly via JAVA_OPTS)
diameter.xmldecoder.appendAvpcode=false
diameter.xmldecoder.appendAvpvendor=false
#diameter-transport configurable settings - end
#diameter interceptors clearing interval (minutes)
diameter.interceptor.clean.interval=5
#diameter connections life time (hours)
diameter.cacheLifetime=12
##=================Monitoring==========================================
management.server.port=${MONITOR_PORT:8090}
management.endpoints.web.exposure.include=${MONITOR_WEB_EXPOSE:prometheus,health,info,env,shutdown}
management.endpoints.web.base-path=${MONITOR_WEB_BASE:/}
management.endpoints.web.path-mapping.prometheus=${MONITOR_WEB_MAP_PROM:metrics}
management.metrics.tags.application=${spring.application.name}
management.metrics.context.size.collect=true
management.metrics.context.size.collect.for.stubs=false
management.metrics.context.size.collect.threshold=0
##===============Hibernate-multi-tenancy=================================
atp.multi-tenancy.enabled=${MULTI_TENANCY_HIBERNATE_ENABLED:false}

##=================== Swagger =======================
springdoc.api-docs.enabled=${SWAGGER_ENABLED:true}

##==================Scheduler======================
userdata.clean.leave_days=${USERDATA_CLEAN_LEAVE_DAYS:7}
userdata.clean.leave_days.timeunit=${USERDATA_CLEAN_LEAVE_DAYS_TIMEUNIT:days}
userdata.clean.job.name=atp-itf-executor-clean-userdata-job
userdata.clean.job.expression=${USERDATA_CLEAN_JOB_EXPRESSION:0 0 3 1/3 * ?}

##=============Audit Logging=================
atp.audit.logging.enable=${AUDIT_LOGGING_ENABLE:false}
atp.audit.logging.topic.name=${AUDIT_LOGGING_TOPIC_NAME:audit_logging_topic}
atp.audit.logging.topic.partitions=${AUDIT_LOGGING_TOPIC_PARTITIONS:1}
atp.audit.logging.topic.replicas=${AUDIT_LOGGING_TOPIC_REPLICAS:1}
atp.reporting.kafka.producer.bootstrap-server=${KAFKA_REPORTING_SERVERS:kafka.reporting.svc:9092}

##==================Javers for ITF History=====================
javers.auditableAspectEnabled=${JAVERS_ENABLED:false}
javers.springDataAuditableRepositoryAspectEnabled=${JAVERS_ENABLED:false}
##=============================History Settings=============================
javers.history.enabled=${JAVERS_HISTORY_ENABLED:true}
##==================Scheduler======================
atp.itf.history.clean.job.name=atp-itf-executor-clean-history-job
atp.itf.history.clean.job.expression=${HISTORY_CLEAN_JOB_EXPRESSION:0 10 0 * * ?}
atp.itf.history.clean.job.revision.max.count=${HISTORY_CLEAN_JOB_REVISION_MAX_COUNT:100}
atp.itf.history.clean.job.page-size=${HISTORY_CLEAN_JOB_PAGE_SIZE:100}
